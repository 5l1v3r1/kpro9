%==================
\chapter{Test Plan}
%==================
This chapter presents the test plan for our solution. The test plan is based on
the standards set by the IEEE829-1998 standard for software testing~\cite{IEEE829}, but with a
few changes to better fit with our project. The purpose of this plan is to have
a structured way of performing tests, as well as providing the developers with
a list of specific component-behaviors. The tests will be based on functional as
well as non-functional requirements, deterring architectural drift and
enforcing our design plans for the system.


%----------------------------
\section{Methods for Testing}
%----------------------------
Regarding software testing, we have two different types of tests available, namely Black box and white box tests. This section is dedicated to the discussion of these two testing methodologies.


%--------------------------
\subsection{White Box Testing}
%--------------------------
White box testing is a method of software testing where you test internal structures or modules of an application, as opposed to its functions. White box testing requires the tester to have an internal perspective of the system, as well as sufficient programming skills. As the \gls{utility} was required to be able to function with a variety of different input, as well as being used as a debugging tool itself, we chose to have every developer on the team write unit tests for their own code, and then have someone else on the team do the testing of their code in order to ensure correctness. Also, in order to get a proper overview over what and how many parts of the system that are covered by unit tests, the team decided to use a tool for measuring code coverage.

\subsubsection{Attest}
As a tool for creating white box unit tests, the team decided to use the Attest testing framework for python code. To create unit tests using Attest, you start off by importing Tests, assert\_hook and optionally contexts from attest. You then create a variable and initialize it to an instance of Tests, which is the variable that will contain list functions that each constitutes one test that is to be run. To feed your test instance with functions for testing you then have to mark these functions with a decorator and feed it the .tests function of the Tests instance. After creating a unit test in this fashion you can run all of your unit tests through Attest from the command line by typing ''python -m attest''. This runs all of your unit tests through Attest and returns a message telling the user how many assertions failed, as well as what input made them fail. For more information read the user documentation of Attest. 

\subsubsection{Coverage}
As a tool for calculating code coverage the team decided to use Coverage, which is a tool for measuring code coverage in python projects. In order to run Coverage from the command line with the tests for this utility, you would have to first navigate to the folder where you installed CSjark, before typing ''Coverage run -m attest''. This generates a file that will be used to Coverage for generating a html table displaying the coverage. In order to create this html table you would then have to type ''coverage html'', which generates a folder named htmlcov. This htmlcov folder again contains a file named index.html that contains a html table describing which parts of the system underwent testing and their code coverage.


%--------------------------
\subsection{Black Box Testing}
%--------------------------
Black box testing is a method of software testing where you test the functionality of a system, as opposed to its internal structures. Black box testing does in general not require the tester to have any intimate knowledge about the system or any of the programming logic that went into making it. Black box test cases are built around the specifications and requirements of a system, for example its functional, and in some cases, non-functional requirements. The team decided to use black box testing for both the functional and non-functional requirements of the \gls{utility}, as the customer had already expressed thoughts on extending and understanding the non-functional parts of the \gls{utility} themselves. 

%--------------------------------------------
\section{Non-Functional Requirements}
%---------------------------------------------
As evaluating the non-functional requirements of a system through its source code is very difficult, it was decided that we should create test cases for them the same way we created black box test cases. Some of these tests would also require the team to have more manpower or resources than what can be expected by a group of students. It was therefore also decided that we would ask the customer for help regarding the testing of some of the non-functional requirements. These test cases would then be designed according to the wishes of the customer and what resources they were able to supply us with.

%------------------------------
\section{Templates for Testing}
%------------------------------
\autoref{tab:testcase} , \autoref{tab:testreport} and \autoref{tab:TestCoverageReport} are templates we will be
using for testing purposes.

In order to standardize the testing process, the team decided on making templates for both the test cases themselves and for reporting their results. The ones responsible for testing were given the task of not only creating and running the test cases themselves, but also adhering to the standards set in this document.

\autoref{tab:testcase} shows the template for each test case. All of the test cases written for the \gls{utility} will be in this format, and executed according to this document. 

\autoref{tab:testreport} shows the template for reporting the result of each test case. \autoref{tab:TestCoverageReport} shows the template for reporting code coverage.

\begin{table}[htbp] \footnotesize \center
\caption{Test Case Template \label{tab:testcase}}
\begin{tabular}{l l}
	\toprule
	Header & Description \\
	\midrule
	Description & Description of requirement \\
	Tester & Team member responsible for the test \\
	Prerequisites & Conditions that needs to be fulfilled before starting the test \\
	Feature & Feature to test \\
	Execution & Steps to be executed in the test \\
	Expected result & The expected output of the test \\
	\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp] \footnotesize \center
\caption{Test Report Template \label{tab:testreport}}
\begin{tabularx}{\textwidth}{l l}
	\toprule
	Header & Description \\
	\midrule
	Test ID & ID for the given test \\
	Description & Description of requirement \\
	Tester & Team member responsible for the test \\
	Date & The date the testing took place \\
	Result & The success or failure of the test, and a comment on the result if needed \\
	\bottomrule
\end{tabularx}
\end{table}

\begin{table}[htbp] \footnotesize \center
	\caption{Code Coverage Report Template\label{tab:TestCoverageReport}}
	\noindent\makebox[\textwidth]{%
	\begin{tabularx}{1.2\textwidth}{l l l l l}
		\toprule
		Module & Statements & Missing & Excluded & Coverage\\
		\midrule
		module 1 & Statements ran & Statements not ran  & Excluded statements & Percent code coverage\ \\
		... & ... & ... & ... & ... \\
		module n & Statements ran & Statements not ran & Excluded statements  & Percent code coverage \\
		\bottomrule
		Total & Statements ran & Statements not ran & Excluded statements & Percent code coverage \\
		\bottomrule
	\end{tabularx}}
\end{table}


%----------------------
\section{Test Criteria}
%----------------------
An item will be considered to have passed a test if the actual result from the test matches the expected result from the test. An item will be considered to have failed the test if the output varies from the expected result. If there are any specifics as to why the test passed/failed, which needs to be discussed, they will be listed as a comment to the result


%---------------------------------
\section{Testing Responsibilities}
%---------------------------------
Each team member is responsible for writing their own unit tests, while the test leader is responsible for the quality of the test plan and the tests.
The tests will mainly not be executed by the same developers who wrote the code that is to undergo testing, but by others in the testing team with as little ownership of the code as possible.

\subsection{Testing Routines}
%---------------------------------
In each sprint we decided that the unit tests would be run continuously throughout the sprints in order to make sure that no new functionality broke any of the earlier code. As the test cases depended somewhat on having completed most of the functionalities for a sprint, as well as requiring a lot of manually generated testing data, it was decided that they would be run towards the end of the sprint. The test results would then be presented a few days before the sprint evaluation, giving the developers some time to fix any bugs discovered by the test cases. 

%------------------
\section{Changelog}
%------------------

\subsection{Sprint 1}
%----------------------
During sprint 1, it became apparent that the customer would not be able to supply the team with any real traffic data to use for testing. The testing team therefore decided to use a hex-editor to generate their own data \glspl{packet} with the \Gls{c}-\glspl{struct} that would be used for testing the \gls{utility}. This is done by manually writing the hex values for the individual bytes in a pcap \gls{packet}, where the first byte indicates the version number, the second byte indicates the flag value of the \gls{packet} and where the third and fourth byte indicates the message ID. The rest of the bytes in the \gls{packet} then contain the \gls{member} values of whatever \gls{struct} was associated with the \gls{packet}'s message ID.

During the sprint, it also became apparent that \Gls{wireshark} would be able to provide the developers and testers with feedback on syntax and user errors on both the \glspl{dissector} created by the \gls{utility}, as well as the traffic used for testing. This is done by \Gls{wireshark} crashing and/or providing the team with error messages related to the code in which the \gls{dissector} is faulty. There will also be displayed a warning or error message with the generated traffic-data if there are any faults with them. The team was therefore able to use \Gls{wireshark} in assisting them in creating correct code early on before writing unit tests.   

\subsection{Sprint 2}
%----------------------
As of sprint 2, it was decided that the team should use an automated tool for calculating code coverage. Code coverage is a measure describing the actual amount of, and which code that undergoes unit testing. As code coverage inspects code directly, it is considered a form of white box testing. In this project it will be used to ensure that an as big part as possible of the system actually undergoes testing, and that the unit tests associated with the different modules of the \gls{utility} actually tests what they are supposed to. It was also added as a goal to have at least 80\% code coverage at the end of the project, where the testers and developers would aim to increase the amount of code coverage from each previous sprint.

\subsection{Sprint 3}
%----------------------
During sprint 3, it was suggested that the team should make a \Gls{c} program that returns \glspl{hex dump}. The \glspl{hex dump} are used for producing traffic to test the \gls{utility} with. It was therefore decided that the team would create such a program to generate data for the bigger and more complex \Gls{c}-\gls{header} files.
These \gls{header} files could have a dozen of \gls{struct}-\glspl{member} of various types. As it would be tedious to have to manually write down the hex values of each \gls{struct} \gls{member}, creating this small program would reduce the time for generating test traffic.

